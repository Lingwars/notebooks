{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "\n",
    " * [¿Qué es WordNet?](Qué-es-WordNet)\n",
    " * [Synsets](#Synsets)\n",
    "    * [Categoría gramatical de un synset](#Categoría-gramatical-de-un-synset)\n",
    "    * [Lo que nos gusta de los synsets](#Lo-que-nos-gusta-de-los-synsets)\n",
    "    * [Lemma](#Lemma)\n",
    " * [Relaciones](#Relaciones)\n",
    "    * [Synset y las relaciones semánticas](#Synset-y-las-relaciones-semánticas)\n",
    "       * [Hiperónimos](#Hiperónimos)\n",
    "       * [Hipónimos](#Hipónimos)\n",
    "       * [Holónimos](#Holónimos)\n",
    "       * [Merónimos](#Merónimos)\n",
    "    * [Lemma y sus relaciones](#Lemma-y-sus-relaciones)\n",
    "       * [Formas derivadas](#Formas-derivadas)\n",
    "       * [Antónimos](#Antónimos)\n",
    " * [Aplicaciones](#Aplicaciones)\n",
    "    * [Expansión de búsquedas](#Expansión-de-búsquedas)\n",
    "    * [Distancia semántica](#Distancia-semántica)\n",
    "    * [Especificidad y concreción](#Especificidad-y-concreción)\n",
    " * [Referencias](#Referencias)\n",
    " \n",
    "Puedes encontrar la última versión de este *workbook* en la cuenta de [Lingẅars](http://lingwars.github.io/blog/) de Github: [https://github.com/Lingwars/notebooks](https://github.com/Lingwars/notebooks/blob/master/Taller%20NLTK%20-%202017/WordNet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet  # This is NLTK's WordNet\n",
    "\n",
    "# Import some helpful classes and functions\n",
    "from IPython.display import HTML, display, Image\n",
    "\n",
    "# This is a function to display a table in HTML format\n",
    "def display_table(data, headers=None, caption=None):\n",
    "    html = [\"<table align=\\\"left\\\">\"]\n",
    "    \n",
    "    if caption:\n",
    "        html += [\"<caption>{}</caption>\".format(caption)]\n",
    "        \n",
    "    if headers:\n",
    "        html += [\"<tr>\"] + [\"<th>{}</th>\".format(h) for h in headers] + [\"</tr>\"]\n",
    "    \n",
    "    for row in data:\n",
    "        html += [\"<tr>\"]\n",
    "        html += [\"<td>{}</td>\".format(it) for it in row]\n",
    "        html += [\"</tr>\"]\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "    display(HTML(''.join(html)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qué es WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WordNet](https://wordnet.princeton.edu/) es una red de conceptos que contiene información codificada manualmente sobre sustantivos, verbos, adjetivos y adverbios en inglés; los términos que representan un mismo concepto están agrupados en *synsets* y son estos elementos los que constituyen los nodos de la red.\n",
    "\n",
    "WordNet se creó en el Laboratorio de Ciencia Cognitiva de la Universidad de Princeton en 1985 bajo la dirección del profesor de psicología George Armitage Miller (1920-2012).\n",
    "\n",
    "Para que nos hagamos una idea de la cantidad de información que contiene ([más números](http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is inside WordNet?\n",
      "There are 82115 nouns.\n",
      "There are 13767 verbs.\n",
      "There are 18156 adjectives.\n",
      "There are 3621 adverbs.\n"
     ]
    }
   ],
   "source": [
    "n_nouns = len(list(wordnet.all_synsets(pos=wordnet.NOUN)))\n",
    "n_verbs = len(list(wordnet.all_synsets(pos=wordnet.VERB)))\n",
    "n_adj = len(list(wordnet.all_synsets(pos=wordnet.ADJ)))\n",
    "n_adv = len(list(wordnet.all_synsets(pos=wordnet.ADV)))\n",
    "\n",
    "print(\"What is inside WordNet?\")\n",
    "print(\"There are {} nouns.\".format(n_nouns))\n",
    "print(\"There are {} verbs.\".format(n_verbs))\n",
    "print(\"There are {} adjectives.\".format(n_adj))\n",
    "print(\"There are {} adverbs.\".format(n_adv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synsets\n",
    "\n",
    "Un synset es un conjunto de palabras de la misma categoría gramatical que hacen referencia a la misma realidad extralingüística y por lo tanto pueden ser intercambiadas en un texto sin afectar al significado. Son elementos semánticamente equivalentes. Así, ocurrirá que las palabras polisémicas aparecerán múltiples veces en *synsets* diferentes.\n",
    "\n",
    "Podemos hacer una búsqueda de uno de estos *synsets* utilizando la función `synsets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 synsets referring to this word:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><th>Synset</th><th>Definition</th></tr><tr><td>dog.n.01</td><td>a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds</td></tr><tr><td>frump.n.01</td><td>a dull unattractive unpleasant girl or woman</td></tr><tr><td>dog.n.03</td><td>informal term for a man</td></tr><tr><td>cad.n.01</td><td>someone who is morally reprehensible</td></tr><tr><td>frank.n.02</td><td>a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll</td></tr><tr><td>pawl.n.01</td><td>a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward</td></tr><tr><td>andiron.n.01</td><td>metal supports for logs in a fireplace</td></tr><tr><td>chase.v.01</td><td>go after with the intent to catch</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_synsets = wordnet.synsets('dog')\n",
    "print(\"There are {} synsets referring to this word:\".format(len(my_synsets)))\n",
    "\n",
    "data = []\n",
    "for synset in my_synsets:\n",
    "    data.append([synset.name(), synset.definition()])\n",
    "\n",
    "display_table(data, [\"Synset\", \"Definition\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos quedarnos con uno de ellos y explorar la cantidad de información que ofrece WordNet una vez que hemos encontrado el *synset* que nos interesa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synset.name: dog.n.01\n",
      "synset.definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "synset.examples:\n",
      "\t + the dog barked all night\n",
      "synset.lemmas:\n",
      "\t + dog\n",
      "\t + domestic_dog\n",
      "\t + Canis_familiaris\n"
     ]
    }
   ],
   "source": [
    "my_synset = my_synsets[0]  # TODO: Prueba con otros: my_synsets[1], my_synsets[2],...\n",
    "print(\"synset.name: {}\".format(my_synset.name()))\n",
    "print(\"synset.definition: {}\".format(my_synset.definition()))\n",
    "\n",
    "print(\"synset.examples:\")\n",
    "for example in my_synset.examples():\n",
    "    print(\"\\t + {}\".format(example))\n",
    "\n",
    "print(\"synset.lemmas:\")\n",
    "for lemma in my_synset.lemmas():\n",
    "    print(\"\\t + {}\".format(lemma.name()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y también podemos buscar los lemmas correspondientes a un *synset* en otros idiomas. Vamos a mostrar aquí sólo un ejemplo porque trataremos este tema más adelante. Véamos cuáles son los relacionados con el *synset* que hemos guardado en la variable `my_synset` de la celda anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the languages available: als, arb, bul, cat, cmn, dan, ell, eng, eus, fas, fin, fra, glg, heb, hrv, ind, ita, jpn, nno, nob, pol, por, qcn, slv, spa, swe, tha, zsm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><th>lang</th><th>lemmas</th></tr><tr><td>eng</td><td>dog</br>domestic_dog</br>Canis_familiaris</td></tr><tr><td>spa</td><td>can</br>perro</td></tr><tr><td>fra</td><td>canis_familiaris</br>chien</td></tr><tr><td>eus</td><td>or</br>txakur</br>zakur</td></tr><tr><td>jpn</td><td>イヌ</br>ドッグ</br>洋犬</br>犬</br>飼犬</br>飼い犬</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "languages = sorted(wordnet.langs())\n",
    "print(\"These are the languages available: {}\".format(', '.join(languages)))\n",
    "\n",
    "selected_languages = ['eng', 'spa', 'fra', 'eus', 'jpn',] # TODO: Prueba con otros idiomas (de la lista)\n",
    "\n",
    "data = []\n",
    "for lang in selected_languages:\n",
    "    data.append([lang, '</br>'.join(my_synset.lemma_names(lang))])\n",
    "\n",
    "display_table(data, headers=[\"lang\", \"lemmas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoría gramatical de un synset\n",
    "\n",
    "En el apartado anterior hemos recuperado todos los *synsets* a partir de una palabra y nos han aparecido significados correspondientes a sustantivos, verbos, adjetivos,... pero se puede afinar un poco más la búsqueda utilizando el *part of speech (pos)*:\n",
    "\n",
    " * `wordnet.VERB`\n",
    " * `wordnet.NOUN`\n",
    " * `wordnet.ADJ`\n",
    " * `wordnet.ADV`\n",
    " \n",
    "¡Vamos a verlo en acción!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets_as_noun: [Synset('commodity.n.01'), Synset('good.n.03'), Synset('sake.n.01'), Synset('good.n.01'), Synset('personal_property.n.01')]\n"
     ]
    }
   ],
   "source": [
    "word = \"bien\"  # TODO: seguro que se te ocurren palabras que puedan aparecer en varias categorías gramaticales\n",
    "lang = \"spa\"  # TODO: estamos buscando en español, pero...\n",
    "\n",
    "synsets_as_noun = wordnet.synsets(word, lang=lang, pos=wordnet.NOUN)\n",
    "synsets_as_verb = wordnet.synsets(word, lang=lang, pos=wordnet.VERB)\n",
    "synsets_as_adj = wordnet.synsets(word, lang=lang, pos=wordnet.ADJ)\n",
    "synsets_as_adv = wordnet.synsets(word, lang=lang, pos=wordnet.ADV)\n",
    "\n",
    "# Vamos a imprimir los resultados\n",
    "print(\"synsets_as_noun: {}\".format(synsets_as_noun))\n",
    "\n",
    "# TODO: ¿Te atreves a mostrar más información sobre ellos con una tabla?:\n",
    "# def synset_table(synsets, title):\n",
    "#    data = []\n",
    "#    for synset in synsets:\n",
    "#        data.append([synset.name(), synset.lemma_names(), synset.definition()])\n",
    "#        \n",
    "#    display_table(data, [\"Synset\", \"Lemmas\", \"Definition\"], caption=title)\n",
    "#\n",
    "#synset_table(synsets_as_noun, title=\"Resultados con: wordnet.NOUN\")\n",
    "#synset_table(synsets_as_verb, title=\"Resultados con: wordnet.VERB\")\n",
    "#synset_table(synsets_as_adj,  title=\"Resultados con: wordnet.ADJ\")\n",
    "#synset_table(synsets_as_adv, title=\"Resultados con: wordnet.ADV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo que nos gusta de los *synsets*\n",
    "\n",
    "Lo que nos gusta de los *synsets* es que permiten referirse a un significado sin ambigüedades. A los ordenadores se les da muy mal resolver ambigüedades, generalmente se les da muy mal todo lo que humanos hacemos con relativa facilidad (entender un mensaje, reconocer imágenes,...), pero realizan muy eficazmente tareas que a nosotros nos cuestan mucho tiempo (búsquedas, ordenación,...).\n",
    "\n",
    "Los ordenadores querrían ver los textos de esta forma, **sin ambigüedades**:\n",
    "\n",
    "```\n",
    "El perro ladra ---> El dog.n.01 bark.v.04\n",
    "```\n",
    "así podrían entenderlo y podríamos hacer inferencias sin equivocarnos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma\n",
    "\n",
    "No debe confundirse un **synset** con un **lemma**, tal y como los identifica WordNet. Recordemos que:\n",
    " \n",
    " * un **synset** está asociado a un significado, que puede representarse en lenguaje natural mediante palabras (lemmas) muy diferentes: perro, dog, can,...\n",
    " * un **lemma** es una palabra de lenguaje natural y, por lo tanto, puede tener varios significados (synsets).\n",
    " \n",
    "Esta diferencia es importantísima tenerla presente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01') -> Lemma('dog.n.01.dog')\n",
      "Synset('dog.n.01') -> Lemma('dog.n.01.domestic_dog')\n",
      "Synset('dog.n.01') -> Lemma('dog.n.01.Canis_familiaris')\n"
     ]
    }
   ],
   "source": [
    "ex_synset = wordnet.synset('dog.n.01')\n",
    "for lemma in ex_synset.lemmas():\n",
    "    print(\"{} -> {}\".format(ex_synset, lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, a traves de un *synset* llegamos a lemmas diferentes, pero todos ellos con el mismo significado. De hecho, el identificador del sysnset `dog.n.01` se mantiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> Lemma('dog.n.01.dog') -> Synset('dog.n.01')\n",
      "dog -> Lemma('frump.n.01.dog') -> Synset('frump.n.01')\n",
      "dog -> Lemma('dog.n.03.dog') -> Synset('dog.n.03')\n",
      "dog -> Lemma('cad.n.01.dog') -> Synset('cad.n.01')\n",
      "dog -> Lemma('frank.n.02.dog') -> Synset('frank.n.02')\n",
      "dog -> Lemma('pawl.n.01.dog') -> Synset('pawl.n.01')\n",
      "dog -> Lemma('andiron.n.01.dog') -> Synset('andiron.n.01')\n",
      "dog -> Lemma('chase.v.01.dog') -> Synset('chase.v.01')\n"
     ]
    }
   ],
   "source": [
    "ex_lemmas = wordnet.lemmas(\"dog\")\n",
    "for lemma in ex_lemmas:\n",
    "    print(\"{} -> {} -> {}\".format(lemma.name(), lemma, lemma.synset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, cuando buscamos una palabra obtenemos varios lemmas, cada uno de ellos asociado a un synset diferente. Se puede observar cómo los identificadores de los synsets son diferentes: `dog.n.01`, `frump.n.01`,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué tal vamos hasta aquí?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bien = \"https://media.giphy.com/media/tqxGgrCnQGsHm/giphy.gif\"\n",
    "mal = \"https://media.giphy.com/media/rn0rRhia7343u/giphy.gif\"\n",
    "\n",
    "# TODO: Selecciona la imagen que corresponda (sustituye la variable 'img' por una de las de arriba)\n",
    "# display(HTML(\"<img src='{}' />\".format(img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relaciones\n",
    "\n",
    "Como decíamos al principio, WordNet es más que un diccionario o un traductor, se trata de una **red de conceptos** que nos permite buscar relaciones entre significados de una forma extremadamente fácil e interesante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset y las relaciones semánticas\n",
    "\n",
    "Los elementos de tipo *synset* definen algunas relaciones que puedes explorar. A continuación te indicamos cuáles creemos que son las más interesantes (puedes consultar la lista completa [aquí](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset)):\n",
    "\n",
    " * hiperónimos\n",
    " * hipónimos\n",
    " * holónimos\n",
    " * merónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Relaciones de hiperonimia e hiponimia</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"489pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 488.74 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184 484.736,-184 484.736,4 -4,4\"/>\n",
       "<!-- furniture.n.01 -->\n",
       "<g id=\"node1\" class=\"node\"><title>furniture.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"195.745\" cy=\"-162\" rx=\"59.5901\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"195.745\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">furniture.n.01</text>\n",
       "</g>\n",
       "<!-- bookcase.n.01 -->\n",
       "<g id=\"node2\" class=\"node\"><title>bookcase.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"61.7445\" cy=\"-90\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.7445\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">bookcase.n.01</text>\n",
       "</g>\n",
       "<!-- furniture.n.01&#45;&gt;bookcase.n.01 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>furniture.n.01&#45;&gt;bookcase.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.327,-146.155C147.563,-135.831 121.006,-121.957 99.4708,-110.708\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.068,-107.593 90.5835,-106.065 97.8264,-113.798 101.068,-107.593\"/>\n",
       "</g>\n",
       "<!-- cabinet.n.01 -->\n",
       "<g id=\"node3\" class=\"node\"><title>cabinet.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"195.745\" cy=\"-90\" rx=\"53.8905\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"195.745\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">cabinet.n.01</text>\n",
       "</g>\n",
       "<!-- furniture.n.01&#45;&gt;cabinet.n.01 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>furniture.n.01&#45;&gt;cabinet.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195.745,-143.697C195.745,-135.983 195.745,-126.712 195.745,-118.112\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.245,-118.104 195.745,-108.104 192.245,-118.104 199.245,-118.104\"/>\n",
       "</g>\n",
       "<!-- table.n.02 -->\n",
       "<g id=\"node4\" class=\"node\"><title>table.n.02</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"313.745\" cy=\"-90\" rx=\"46.2923\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.745\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">table.n.02</text>\n",
       "</g>\n",
       "<!-- furniture.n.01&#45;&gt;table.n.02 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>furniture.n.01&#45;&gt;table.n.02</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221.64,-145.638C239.014,-135.332 262.051,-121.666 280.739,-110.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"282.705,-113.483 289.519,-105.371 279.133,-107.463 282.705,-113.483\"/>\n",
       "</g>\n",
       "<!-- altar.n.01 -->\n",
       "<g id=\"node5\" class=\"node\"><title>altar.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"236.745\" cy=\"-18\" rx=\"44.393\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"236.745\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">altar.n.01</text>\n",
       "</g>\n",
       "<!-- table.n.02&#45;&gt;altar.n.01 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>table.n.02&#45;&gt;altar.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M296.272,-73.1159C286.033,-63.808 272.932,-51.8981 261.629,-41.6222\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"263.902,-38.9582 254.148,-34.8212 259.193,-44.1378 263.902,-38.9582\"/>\n",
       "</g>\n",
       "<!-- table&#45;tennis_table.n.01 -->\n",
       "<g id=\"node6\" class=\"node\"><title>table&#45;tennis_table.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"389.745\" cy=\"-18\" rx=\"90.9839\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"389.745\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">table&#45;tennis_table.n.01</text>\n",
       "</g>\n",
       "<!-- table.n.02&#45;&gt;table&#45;tennis_table.n.01 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>table.n.02&#45;&gt;table&#45;tennis_table.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.99,-73.1159C340.717,-64.1572 353.06,-52.788 363.92,-42.7853\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"366.523,-45.1466 371.507,-35.7973 361.781,-39.9977 366.523,-45.1466\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f46c34728d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz  # pip install graphviz (es una librería para pintar grafos)\n",
    "\n",
    "display(HTML(\"<strong>Relaciones de hiperonimia e hiponimia</strong>\"))\n",
    "d = graphviz.Digraph()\n",
    "d.edge(\"furniture.n.01\", \"bookcase.n.01\")\n",
    "d.edge(\"furniture.n.01\", \"cabinet.n.01\")\n",
    "d.edge(\"furniture.n.01\", \"table.n.02\")\n",
    "d.edge(\"table.n.02\", \"altar.n.01\")\n",
    "d.edge(\"table.n.02\", \"table-tennis_table.n.01\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La hiperonimia e hiponimia codifican relaciones a nivel de significado. Un **hipónimo** concreta el significado de su **hiperónimo**, así mesa es más específico que mueble, como altar es un tipo particular de mesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Relaciones de holonimia y meronimia</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('armrest.n.01'), Synset('doorlock.n.01'), Synset('hinge.n.01')]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"450pt\" height=\"152pt\"\n",
       " viewBox=\"0.00 0.00 450.47 152.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 148)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-148 446.467,-148 446.467,4 -4,4\"/>\n",
       "<!-- car.n.01 -->\n",
       "<g id=\"node1\" class=\"node\"><title>car.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.6465\" cy=\"-72\" rx=\"39.7935\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.6465\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">car.n.01</text>\n",
       "</g>\n",
       "<!-- air_bag.n.01 -->\n",
       "<g id=\"node2\" class=\"node\"><title>air_bag.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"201.085\" cy=\"-126\" rx=\"55.4913\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.085\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">air_bag.n.01</text>\n",
       "</g>\n",
       "<!-- car.n.01&#45;&gt;air_bag.n.01 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>car.n.01&#45;&gt;air_bag.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.0598,-83.3028C84.5648,-88.2436 100.679,-94.0325 115.293,-99 126.72,-102.884 139.07,-106.913 150.715,-110.635\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"149.704,-113.986 160.295,-113.68 151.825,-107.315 149.704,-113.986\"/>\n",
       "</g>\n",
       "<!-- car_door.n.01 -->\n",
       "<g id=\"node3\" class=\"node\"><title>car_door.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"201.085\" cy=\"-72\" rx=\"59.5901\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.085\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">car_door.n.01</text>\n",
       "</g>\n",
       "<!-- car.n.01&#45;&gt;car_door.n.01 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>car.n.01&#45;&gt;car_door.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79.6496,-72C95.1398,-72 113.434,-72 131.023,-72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"131.271,-75.5001 141.271,-72 131.271,-68.5001 131.271,-75.5001\"/>\n",
       "</g>\n",
       "<!-- gasoline_engine.n.01 -->\n",
       "<g id=\"node4\" class=\"node\"><title>gasoline_engine.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"201.085\" cy=\"-18\" rx=\"85.5853\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.085\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">gasoline_engine.n.01</text>\n",
       "</g>\n",
       "<!-- car.n.01&#45;&gt;gasoline_engine.n.01 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>car.n.01&#45;&gt;gasoline_engine.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.0598,-60.6972C84.5648,-55.7564 100.679,-49.9675 115.293,-45 123.994,-42.0424 133.229,-39.0014 142.281,-36.0756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.546,-39.3454 151.997,-32.9553 141.406,-32.6806 143.546,-39.3454\"/>\n",
       "</g>\n",
       "<!-- doorlock.n.01 -->\n",
       "<g id=\"node5\" class=\"node\"><title>doorlock.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"382.672\" cy=\"-99\" rx=\"59.5901\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.672\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">doorlock.n.01</text>\n",
       "</g>\n",
       "<!-- car_door.n.01&#45;&gt;doorlock.n.01 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>car_door.n.01&#45;&gt;doorlock.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254.996,-79.9549C275.002,-82.9626 297.964,-86.4149 318.796,-89.5469\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"318.421,-93.0297 328.83,-91.0554 319.461,-86.1075 318.421,-93.0297\"/>\n",
       "</g>\n",
       "<!-- hinge.n.01 -->\n",
       "<g id=\"node6\" class=\"node\"><title>hinge.n.01</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"382.672\" cy=\"-45\" rx=\"48.1917\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.672\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">hinge.n.01</text>\n",
       "</g>\n",
       "<!-- car_door.n.01&#45;&gt;hinge.n.01 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>car_door.n.01&#45;&gt;hinge.n.01</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254.996,-64.0451C277.888,-60.6034 304.652,-56.5797 327.665,-53.1197\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.243,-56.5723 337.611,-51.6244 327.202,-49.6501 328.243,-56.5723\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f46c34724a8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(HTML(\"<strong>Relaciones de holonimia y meronimia</strong>\"))\n",
    "\n",
    "d = graphviz.Digraph()\n",
    "d.graph_attr['rankdir'] = 'LR'\n",
    "d.edge(\"car.n.01\", \"air_bag.n.01\")\n",
    "d.edge(\"car.n.01\", \"car_door.n.01\")\n",
    "d.edge(\"car.n.01\", \"gasoline_engine.n.01\")\n",
    "d.edge(\"car_door.n.01\", \"doorlock.n.01\")\n",
    "d.edge(\"car_door.n.01\", \"hinge.n.01\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las relaciones de holonimia y meronimia codifican relaciones entre el todo y sus partes. Un puerta (de coche) es una parte del coche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_synset = wordnet.synset(\"hand.n.01\")  # TODO: Cambia el valor almacenado en la variable 'my_synset' para \n",
    "                                         #   obtener otros resultados en las siguientes celdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Hypernyms for Synset('hand.n.01')</caption><tr><th>hyp-synset</th><th>lemmas</th><th>examples</th></tr><tr><td>Synset('extremity.n.05')</td><td>extremity</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "for hypernym in my_synset.hypernyms():\n",
    "    lemmas = [lemma.name() for lemma in hypernym.lemmas()]\n",
    "    data.append([hypernym, ', '.join(lemmas), '</br>'.join(hypernym.examples())])\n",
    "display_table(data, [\"hyp-synset\", \"lemmas\", \"examples\"], caption=\"Hypernyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de los hiperónimos a nivel de significado, también existen **a nivel de instancia `instance_hypernyms`** (igual con los hipónimos). Por ejemplo, si hemos encontrado en un texto una entidad (ver NER) como `Vargas Llosa`, gracias a WordNet podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había buscado: Zweig\n",
      "zweig.n.01 is a writer.n.01\n",
      "Otras formas de llamarlo son: Zweig, Stefan_Zweig\n",
      "Su 'definición' es: Austrian writer (1881-1942)\n"
     ]
    }
   ],
   "source": [
    "q = \"Zweig\"  # TODO: Prueba otros como p.ej.: Vargas_Llosa, Zweig, Einstein\n",
    "\n",
    "ner = wordnet.synsets(q)[0]  # Recoge el primer resultado\n",
    "hiperonimos = ner.instance_hypernyms()[0]  # Busca sus hiperónimos y quédate con el primero\n",
    "\n",
    "# Qué tengo?\n",
    "print(\"Había buscado: {}\".format(q))\n",
    "print(\"{} is a {}\".format(ner.name(), hiperonimos.name()))  # Su hiperónimo me dice su profesión!!\n",
    "print(\"Otras formas de llamarlo son: {}\".format(', '.join([it.name() for it in ner.lemmas()])))\n",
    "print(\"Su 'definición' es: {}\".format(ner.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos casos, por ejemplo, el hiperónimo de un un escritor concreto es la categoría *escritor* puesto que generaliza su significado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hipónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Hyponyms for Synset('hand.n.01')</caption><tr><th>hyponym-synset</th><th>lemmas</th><th>examples</th></tr><tr><td>Synset('fist.n.01')</td><td>fist, clenched_fist</td><td></td></tr><tr><td>Synset('hooks.n.01')</td><td>hooks, meat_hooks, maulers</td><td>wait till I get my hooks on him</td></tr><tr><td>Synset('left.n.03')</td><td>left, left_hand</td><td>jab with your left</td></tr><tr><td>Synset('right.n.05')</td><td>right, right_hand</td><td>he writes with his right hand but pitches with his left</br>hit him with quick rights to the body</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "for hyponym in my_synset.hyponyms():\n",
    "    lemmas = [lemma.name() for lemma in hyponym.lemmas()]\n",
    "    data.append([hyponym, ', '.join(lemmas), '</br>'.join(hyponym.examples())])\n",
    "display_table(data, [\"hyponym-synset\", \"lemmas\", \"examples\"], caption=\"Hyponyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Holonyms for Synset('hand.n.01')</caption><tr><th>holonym-synset</th><th>lemmas</th><th>examples</th></tr><tr><td>Synset('arm.n.01')</td><td>arm</td><td></td></tr><tr><td>Synset('homo.n.02')</td><td>homo, man, human_being, human</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# member_holonyms, substance_holonyms, part_holonyms\n",
    "data = []\n",
    "for holonym in my_synset.part_holonyms():\n",
    "    lemmas = [lemma.name() for lemma in holonym.lemmas()]\n",
    "    data.append([holonym, ', '.join(lemmas), '</br>'.join(holonym.examples())])\n",
    "display_table(data, [\"holonym-synset\", \"lemmas\", \"examples\"], caption=\"Holonyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Meronyms for Synset('hand.n.01')</caption><tr><th>meronym-synset</th><th>lemmas</th><th>examples</th></tr><tr><td>Synset('ball.n.10')</td><td>ball</td><td>the ball at the base of the thumb</br>he stood on the balls of his feet</td></tr><tr><td>Synset('digital_arteries.n.01')</td><td>digital_arteries, arteria_digitalis</td><td></td></tr><tr><td>Synset('finger.n.01')</td><td>finger</td><td>her fingers were long and thin</td></tr><tr><td>Synset('intercapitular_vein.n.01')</td><td>intercapitular_vein, vena_intercapitalis</td><td></td></tr><tr><td>Synset('metacarpal_artery.n.01')</td><td>metacarpal_artery, arteria_metacarpea</td><td></td></tr><tr><td>Synset('metacarpal_vein.n.01')</td><td>metacarpal_vein, vena_metacarpus</td><td></td></tr><tr><td>Synset('metacarpus.n.01')</td><td>metacarpus</td><td></td></tr><tr><td>Synset('palm.n.01')</td><td>palm, thenar</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# member_meronyms, substance_meronyms, part_meronyms\n",
    "data = []\n",
    "for meronym in my_synset.part_meronyms():\n",
    "    lemmas = [lemma.name() for lemma in meronym.lemmas()]\n",
    "    data.append([meronym, ', '.join(lemmas), '</br>'.join(meronym.examples())])\n",
    "display_table(data, [\"meronym-synset\", \"lemmas\", \"examples\"], caption=\"Meronyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma y sus relaciones\n",
    "\n",
    "Los lemas, por su parte, también definen unas cuantas relaciones interesantes (lista completa [aquí](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Lemma)), si bien, las dos que nos parecen más interesantes son:\n",
    "\n",
    " * antonyms\n",
    " * derivationally_related_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_lemma = wordnet.lemma(\"fast.a.01.fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formas derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Derivates for Lemma('fast.a.01.fast')</caption><tr><th>derivationally-related-forms</th><th>lemmas</th><th>examples</th></tr><tr><td>Lemma('speed.n.02.fastness')</td><td>rapidez</br>velocidad</td><td>the project advanced with gratifying speed</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang='spa'  # TODO: Change the language\n",
    "data = []\n",
    "for item in my_lemma.derivationally_related_forms():\n",
    "    lemmas = item.synset().lemma_names(lang=lang)\n",
    "    data.append([item, '</br>'.join(lemmas), '</br>'.join(item.synset().examples())])\n",
    "display_table(data, [\"derivationally-related-forms\", \"lemmas\", \"examples\"], caption=\"Derivates for {}\".format(my_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Antonym for Lemma('fast.a.01.fast')</caption><tr><th>antonym-lemma</th><th>lemmas</th><th>examples</th></tr><tr><td>Lemma('slow.a.01.slow')</td><td>slow</td><td>a slow walker</br>the slow lane of traffic</br>her steps were slow</br>he was slow in reacting to the news</br>slow but steady growth</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang='eng'  # TODO: Change the language\n",
    "data = []\n",
    "for item in my_lemma.antonyms():\n",
    "    lemmas = item.synset().lemma_names(lang=lang)\n",
    "    data.append([item, '</br>'.join(lemmas), '</br>'.join(item.synset().examples())])\n",
    "display_table(data, [\"antonym-lemma\", \"lemmas\", \"examples\"], caption=\"Antonym for {}\".format(my_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las relaciones entre un elemento y sus vecinos permiten explorar la red de conceptos buscando términos relacionados con una palabra (lema) dada o bien con un concepto (synset) determinado. Sin embargo, esta estructura de relaciones nos permite también abordar problemas mucho más ambiciosos. Algunos de ellos los vemos a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Expansión de búsquedas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las primeras aplicaciones que se nos pueden ocurrir y quizá de las más utilizadas es la expansión de búsquedas. Para ello podemos utilizar estas redes de conceptos para ampliar la búsqueda utilizando otras palabras relacionadas. De este modo lo que se persigue es encontrar documentos que, aunque no contengan exactamente la misma palabra que se ha introducido, sí sean relevantes por contener otras relacionadas.\n",
    "\n",
    "Por ejemplo, si el usuario ha introducido `dalmatian` como término de búsqueda, podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: dalmatian\n",
      "Expanded to synsets: carriage_dog, coach_dog, Dalmatian, dalmatian\n",
      "Expanded with hyperonyms: European, coach_dog, Dalmatian, dog, Canis_familiaris, carriage_dog, domestic_dog, dalmatian\n"
     ]
    }
   ],
   "source": [
    "q = \"dalmatian\"\n",
    "print(\"Original query: {}\".format(q))\n",
    "\n",
    "# Look synsets related to the keyword\n",
    "synsets = wordnet.synsets(q)\n",
    "\n",
    "# Expand the query to all related synsets\n",
    "def gather_lemmas(synset_list):\n",
    "    lemmas = []\n",
    "    for synset in synset_list:\n",
    "        lemmas += synset.lemma_names()\n",
    "    return lemmas\n",
    "    \n",
    "expanded_query = [q] + gather_lemmas(synsets)\n",
    "print(\"Expanded to synsets: {}\".format(', '.join(set(expanded_query))))\n",
    "\n",
    "# Expand the query to all hyperonyms:\n",
    "for synset in synsets:\n",
    "    expanded_query += gather_lemmas(synset.hypernyms())\n",
    "\n",
    "print(\"Expanded with hyperonyms: {}\".format(', '.join(set(expanded_query))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al tener muchos más términos de búsqueda podremos recuperar muchos más documentos de nuestro corpus en caso de que la búsqueda original ofreciera un numero insuficiente de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra aplicación muy habitual cuando se dispone de una red de conceptos es medir la distancia semántica entre significados. Algunas situaciones en las que puede plantearse esta necesidad son la evaluación de traductores (cuál se ha separado menos del significado original) o la desambiguación (ante un mismo lema con varios significados podemos asignarle una probabilidad a cada uno de ellos según la distancia a la temática del documento).\n",
    "\n",
    "Esta aplicación es tan demandada que NLTK implementa los principales algoritmos para calcular esta medida. Por ejemplo, dados tres significados `dog.n.01`, `cat.n.01` y `tiger.n.01`, veamos cuál es la distancia entre ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between Synset('dog.n.01') and Synset('cat.n.01') using the given algorithm is 0.2000\n"
     ]
    }
   ],
   "source": [
    "synset1 = wordnet.synset(\"dog.n.01\")\n",
    "synset2 = wordnet.synset(\"cat.n.01\")\n",
    "sim = wordnet.path_similarity(synset1, synset2)\n",
    "\n",
    "print(\"The similarity between {} and {} using the given algorithm is {:0.4f}\".format(synset1, synset2, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir ahora una lista en las que incorporaremos todas las funciones que ofrece WordNet para el cálculo de la similaridad entre dos *synsets*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Métodos básicos para calcular similitud entre dos términos\n",
    "methods = [('path_similarity', wordnet.path_similarity),\n",
    "           ('Leacock-Chodorow', wordnet.lch_similarity),\n",
    "           ('Wu-Palmer', wordnet.wup_similarity),]\n",
    "\n",
    "# Algunos algoritmos necesitan un 'corpus' para utilizarlo como referencia\n",
    "from nltk.corpus import wordnet_ic, genesis\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "genesis_ic = wordnet.ic(genesis, False, 0.0) # Esto puede tardar un poco la primera vez\n",
    "\n",
    "methods_ic = [('Resnik + Brown', lambda u,v: wordnet.res_similarity(u, v, brown_ic)),\n",
    "              ('Resnik + Semcor', lambda u,v: wordnet.res_similarity(u, v, semcor_ic)),\n",
    "              ('Resnik + Genesis', lambda u,v: wordnet.res_similarity(u, v, genesis_ic)),\n",
    "                \n",
    "              ('Jiang-Conrath + Brown', lambda u,v: wordnet.jcn_similarity(u, v, brown_ic)),\n",
    "              ('Jiang-Conrath + Semcor', lambda u,v: wordnet.jcn_similarity(u, v, semcor_ic)),\n",
    "              ('Jiang-Conrath + Genesis', lambda u,v: wordnet.jcn_similarity(u, v, genesis_ic)),\n",
    "           \n",
    "              ('Lin + Brown', lambda u,v: wordnet.lin_similarity(u, v, brown_ic)),\n",
    "              ('Lin + Semcor', lambda u,v: wordnet.lin_similarity(u, v, semcor_ic)),\n",
    "              ('Lin + Genesis', lambda u,v: wordnet.lin_similarity(u, v, genesis_ic)),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también una función para mostrar los resultados de nuestros cálculos en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float_format = \"{0:.4f}\"\n",
    "\n",
    "def compute_distances(item_list, method_list):\n",
    "    data = []\n",
    "    it1 = item_list[0]\n",
    "    for key,method in method_list:\n",
    "        max_similarity = method(it1, it1)\n",
    "        row = [float_format.format(1.0),]\n",
    "\n",
    "        for it2 in item_list[1:]:\n",
    "            similitud = method(it1, it2)/max_similarity  # Normalized by 'similarity(it1, it1)'\n",
    "            row.append(float_format.format(similitud)) \n",
    "        data.append([key] + row)\n",
    "\n",
    "    columns = [\"{} > {}\".format(item_list[0].name(), it.name()) for it in item_list]\n",
    "    display_table(data, [\"Method\"] + columns, caption=\"Similitud normalizada entre '{}'\".format(it1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los elementos anteriores, la lista de algoritmos y la función auxiliar, podemos empezar a calcular cosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Similitud normalizada entre 'Synset('dog.n.01')'</caption><tr><th>Method</th><th>dog.n.01 > dog.n.01</th><th>dog.n.01 > cat.n.01</th><th>dog.n.01 > tiger.n.01</th></tr><tr><td>path_similarity</td><td>1.0000</td><td>0.2000</td><td>0.1667</td></tr><tr><td>Leacock-Chodorow</td><td>1.0000</td><td>0.5576</td><td>0.5074</td></tr><tr><td>Wu-Palmer</td><td>1.0000</td><td>0.9231</td><td>0.7602</td></tr><tr><td>Resnik + Brown</td><td>1.0000</td><td>0.8785</td><td>0.2470</td></tr><tr><td>Resnik + Semcor</td><td>1.0000</td><td>0.9373</td><td>0.2355</td></tr><tr><td>Resnik + Genesis</td><td>1.0000</td><td>0.7236</td><td>0.1445</td></tr><tr><td>Jiang-Conrath + Brown</td><td>1.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>Jiang-Conrath + Semcor</td><td>1.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>Jiang-Conrath + Genesis</td><td>1.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>Lin + Brown</td><td>1.0000</td><td>0.8768</td><td>0.2091</td></tr><tr><td>Lin + Semcor</td><td>1.0000</td><td>0.8863</td><td>0.1869</td></tr><tr><td>Lin + Genesis</td><td>1.0000</td><td>0.8044</td><td>0.0000</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dog = wordnet.synset(\"dog.n.01\")\n",
    "cat = wordnet.synset(\"cat.n.01\")\n",
    "tiger = wordnet.synset(\"tiger.n.01\")\n",
    "animals = [dog, cat, tiger] # TODO: Aquí puedes añadir más synsets\n",
    "\n",
    "compute_distances(animals, methods + methods_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einstein is a physicist\n",
      "Austen is a writer\n",
      "Zweig is a writer\n",
      "Cousteau is a explorer\n",
      "Akhenaton is a king\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Similitud normalizada entre 'Synset('einstein.n.01')'</caption><tr><th>Method</th><th>einstein.n.01 > einstein.n.01</th><th>einstein.n.01 > austen.n.01</th><th>einstein.n.01 > zweig.n.01</th><th>einstein.n.01 > cousteau.n.01</th><th>einstein.n.01 > akhenaton.n.01</th></tr><tr><td>path_similarity</td><td>1.0000</td><td>0.1429</td><td>0.1429</td><td>0.1667</td><td>0.1250</td></tr><tr><td>Leacock-Chodorow</td><td>1.0000</td><td>0.4651</td><td>0.4651</td><td>0.5074</td><td>0.4283</td></tr><tr><td>Wu-Palmer</td><td>1.0000</td><td>0.6000</td><td>0.6000</td><td>0.6316</td><td>0.5714</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><caption>Similitud normalizada entre 'Synset('zweig.n.01')'</caption><tr><th>Method</th><th>zweig.n.01 > zweig.n.01</th><th>zweig.n.01 > einstein.n.01</th><th>zweig.n.01 > austen.n.01</th><th>zweig.n.01 > cousteau.n.01</th><th>zweig.n.01 > akhenaton.n.01</th></tr><tr><td>path_similarity</td><td>1.0000</td><td>0.1429</td><td>0.3333</td><td>0.1667</td><td>0.1250</td></tr><tr><td>Leacock-Chodorow</td><td>1.0000</td><td>0.4651</td><td>0.6980</td><td>0.5074</td><td>0.4283</td></tr><tr><td>Wu-Palmer</td><td>1.0000</td><td>0.6000</td><td>0.6000</td><td>0.6316</td><td>0.5714</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = wordnet.synset(\"Einstein.n.01\")\n",
    "p2 = wordnet.synset(\"Austen.n.01\")\n",
    "p3 = wordnet.synset(\"Zweig.n.01\")\n",
    "p4 = wordnet.synset(\"Cousteau.n.01\")\n",
    "p5 = wordnet.synset(\"akhenaton.n.01\")\n",
    "people = [p1, p2, p3, p4, p5] # TODO: Aquí puedes añadir más synsets\n",
    "\n",
    "for it in people:\n",
    "    hyp = it.instance_hypernyms()[0]\n",
    "    print(\"{} is a {}\".format(it.lemmas()[0].name(), hyp.lemmas()[0].name()))\n",
    "\n",
    "compute_distances([p1, p2, p3, p4, p5], methods)\n",
    "compute_distances([p3, p1, p2, p4, p5], methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desambiguación\n",
    "\n",
    "El problema clásico y ¿sin solución definitiva? del Procesamiento de Lenguaje Natural: desambiguar el significado de una palabra. Una forma de abordarlo que se me ocurre con lo que sabemos hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"El banco presta dinero a cambio de un interés\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos tokenizar y realizar el análisis sintáctico de una oración. Con ello obtendremos las palabras individuales y el *part of speech* de cada una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = [(\"bank\", \"n\"), (\"lend\", \"v\"), (\"money\", \"n\"), (\"interest\", \"n\")]\n",
    "\n",
    "# Remove everything but nouns (WordNet does not retrieve similarity for different part of speech synsets)\n",
    "sentence = [(it, pos) for it, pos in sentence if pos==\"n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente tenemos un problema de desambiguación puesto que cada una de estas palabras puede tener diferentes significados. Vamos a listarlos todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Significados de bank:</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><td>Synset('bank.n.01')</td><td>bank</td><td>sloping land (especially the slope beside a body of water)</td></tr><tr><td>Synset('depository_financial_institution.n.01')</td><td>depository_financial_institution, bank, banking_concern, banking_company</td><td>a financial institution that accepts deposits and channels the money into lending activities</td></tr><tr><td>Synset('bank.n.03')</td><td>bank</td><td>a long ridge or pile</td></tr><tr><td>Synset('bank.n.04')</td><td>bank</td><td>an arrangement of similar objects in a row or in tiers</td></tr><tr><td>Synset('bank.n.05')</td><td>bank</td><td>a supply or stock held in reserve for future use (especially in emergencies)</td></tr><tr><td>Synset('bank.n.06')</td><td>bank</td><td>the funds held by a gambling house or the dealer in some gambling games</td></tr><tr><td>Synset('bank.n.07')</td><td>bank, cant, camber</td><td>a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force</td></tr><tr><td>Synset('savings_bank.n.02')</td><td>savings_bank, coin_bank, money_box, bank</td><td>a container (usually with a slot in the top) for keeping money at home</td></tr><tr><td>Synset('bank.n.09')</td><td>bank, bank_building</td><td>a building in which the business of banking transacted</td></tr><tr><td>Synset('bank.n.10')</td><td>bank</td><td>a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Significados de money:</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><td>Synset('money.n.01')</td><td>money</td><td>the most common medium of exchange; functions as legal tender</td></tr><tr><td>Synset('money.n.02')</td><td>money</td><td>wealth reckoned in terms of money</td></tr><tr><td>Synset('money.n.03')</td><td>money</td><td>the official currency issued by a government or national bank</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Significados de interest:</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><td>Synset('interest.n.01')</td><td>interest, involvement</td><td>a sense of concern with and curiosity about someone or something</td></tr><tr><td>Synset('sake.n.01')</td><td>sake, interest</td><td>a reason for wanting something done</td></tr><tr><td>Synset('interest.n.03')</td><td>interest, interestingness</td><td>the power of attracting or holding one's attention (because it is unusual or exciting etc.)</td></tr><tr><td>Synset('interest.n.04')</td><td>interest</td><td>a fixed charge for borrowing money; usually a percentage of the amount borrowed</td></tr><tr><td>Synset('interest.n.05')</td><td>interest, stake</td><td>(law) a right or legal share of something; a financial involvement with something</td></tr><tr><td>Synset('interest.n.06')</td><td>interest, interest_group</td><td>(usually plural) a social group whose members control some field of activity and who have common aims</td></tr><tr><td>Synset('pastime.n.01')</td><td>pastime, interest, pursuit</td><td>a diversion that occupies one's time and thoughts (usually pleasantly)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word, pos in sentence:\n",
    "    display(HTML(\"<strong>Significados de {}:</strong>\".format(word)))\n",
    "\n",
    "    synsets = wordnet.synsets(word, pos=pos)\n",
    "    data = []\n",
    "    for s in synsets:\n",
    "        data.append([s, ', '.join([it.name() for it in s.lemmas()]), s.definition()])\n",
    "        \n",
    "    display_table(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La estrategia que vamos a seguir es probar todas las combinaciones posibles y quedarnos con aquélla que ofrezca la mayor similitud entre sus componentes. ¿Aproximación válida? ¿Inválida? ¿Muy costosa computacionalmente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "There are 210 possible combinations!!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Top 10 are:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><th>score</th><th>bank</th><th>money</th><th>interest</th></tr><tr><td>0.4857</td><td>bank.n.06: bank</td><td>money.n.02: money</td><td>interest.n.05: interest, stake</td></tr><tr><td>0.3818</td><td>bank.n.06: bank</td><td>money.n.01: money</td><td>interest.n.05: interest, stake</td></tr><tr><td>0.3778</td><td>bank.n.04: bank</td><td>money.n.02: money</td><td>interest.n.06: interest, interest_group</td></tr><tr><td>0.3778</td><td>bank.n.04: bank</td><td>money.n.01: money</td><td>interest.n.06: interest, interest_group</td></tr><tr><td>0.3667</td><td>bank.n.06: bank</td><td>money.n.03: money</td><td>interest.n.05: interest, stake</td></tr><tr><td>0.3651</td><td>bank.n.06: bank</td><td>money.n.02: money</td><td>interest.n.06: interest, interest_group</td></tr><tr><td>0.3611</td><td>depository_financial_institution.n.01: depository_financial_institution, bank, banking_concern, banking_company</td><td>money.n.01: money</td><td>interest.n.06: interest, interest_group</td></tr><tr><td>0.3611</td><td>depository_financial_institution.n.01: depository_financial_institution, bank, banking_concern, banking_company</td><td>money.n.02: money</td><td>interest.n.06: interest, interest_group</td></tr><tr><td>0.3576</td><td>bank.n.04: bank</td><td>money.n.03: money</td><td>interest.n.06: interest, interest_group</td></tr><tr><td>0.3436</td><td>depository_financial_institution.n.01: depository_financial_institution, bank, banking_concern, banking_company</td><td>money.n.03: money</td><td>interest.n.06: interest, interest_group</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>And the winner is</strong>, with a score of 0.4857142857142857:<ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<li><strong>bank.n.06</strong>: the funds held by a gambling house or the dealer in some gambling games</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<li><strong>money.n.02</strong>: wealth reckoned in terms of money</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<li><strong>interest.n.05</strong>: (law) a right or legal share of something; a financial involvement with something</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "</ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_options = [wordnet.synsets(word, pos=pos) for word, pos in sentence]\n",
    "\n",
    "import itertools\n",
    "all_combinations = list(itertools.product(*sentence_options))\n",
    "display(HTML(\"There are {} possible combinations!!\".format(len(all_combinations))))\n",
    "\n",
    "data = {}\n",
    "for comb in all_combinations:\n",
    "    sim = 0\n",
    "    for pair in itertools.combinations(comb, r=2):\n",
    "        if pair[0].pos() == pair[1].pos():  # WordNet does not return similarity between different part-of-speech\n",
    "            sim += wordnet.path_similarity(pair[0], pair[1])\n",
    "    data[comb] = sim\n",
    "\n",
    "import operator\n",
    "sorted_data = sorted(data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Keep just the first 10\n",
    "display(HTML(\"Top 10 are:\"))\n",
    "top10 = sorted_data[:10]\n",
    "\n",
    "# Vamos a imprimir los lemmas, aunque un ordenador lo que querría son los synsets.\n",
    "data_to_display = []\n",
    "for it, sim in top10:\n",
    "    row = [\"{:0.4f}\".format(sim)]\n",
    "    for synset in it:\n",
    "        cell_text = \"{}: {}\".format(synset.name(), ', '.join([lema.name() for lema in synset.lemmas()]))\n",
    "        row.append(cell_text)\n",
    "    data_to_display.append(row)\n",
    "\n",
    "display_table(data_to_display, headers=[\"score\"] + [word for word, pos in sentence])\n",
    "\n",
    "# And the winner is\n",
    "winner, sim = top10[0]\n",
    "display(HTML(\"<strong>And the winner is</strong>, with a score of {}:<ul>\".format(sim)))\n",
    "for it in winner:\n",
    "    display(HTML(\"<li><strong>{}</strong>: {}</li>\".format(it.name(), it.definition())))\n",
    "display(HTML(\"</ul>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo podemos mejorar este resultado de manera fácil? Si tenemos información sobre el contexto, por ejemplo, si sabemos que es una noticia de la sección económica de un periódico, entonces podemos utilizar como input la frecuencia relativa de nuestros significados dentro de ese corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificidad y concreción\n",
    "\n",
    "¡Toma epígrage! Hay un método que te devuelve la profundidad en la jerarquía de conceptos de una palabra, ¿se podría evaluar en base a esto cómo de riguroso es un autor?\n",
    "\n",
    "En primer lugar, ten en cuenta que WordNet no es una estructura en forma de árbol, sino que es un grafo, es decir, ¡puede haber varios caminos para llegar a un mismo nodo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Path:\n",
      "entity.n.01 >> physical_entity.n.01 >> object.n.01 >> whole.n.02 >> living_thing.n.01 >> organism.n.01 >> animal.n.01 >> chordate.n.01 >> vertebrate.n.01 >> mammal.n.01 >> placental.n.01 >> carnivore.n.01 >> canine.n.02 >> dog.n.01\n",
      "== Path:\n",
      "entity.n.01 >> physical_entity.n.01 >> object.n.01 >> whole.n.02 >> living_thing.n.01 >> organism.n.01 >> animal.n.01 >> domestic_animal.n.01 >> dog.n.01\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4ccaed4ad94c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' >> '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.4/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mtree\u001b[0;34m(self, rel, depth, cut_mark)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_mark\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcut_mark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcut_mark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "dog = wordnet.synset('dog.n.01')\n",
    "\n",
    "for path in dog.hypernym_paths():\n",
    "    print(\"== Path:\")\n",
    "    print(' >> '.join([item.name() for item in path]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es importante porque al calcular la profundidad de un nodo en la jerarquía de conceptos obtendremos varios valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\"><tr><th>synset</th><th>min_depth</th><th>max_depth</th></tr><tr><td>animal</td><td>6</td><td>6</td></tr><tr><td>dalmatian</td><td>6</td><td>9</td></tr><tr><td>dog</td><td>8</td><td>13</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = {'dog': 'dog.n.01',\n",
    "          'dalmatian': 'dalmatian.n.01',\n",
    "          'animal': 'animal.n.01'}\n",
    "\n",
    "data = []\n",
    "for w,s in words.items():\n",
    "    min_depth = wordnet.synset(s).min_depth()\n",
    "    max_depth = wordnet.synset(s).max_depth()\n",
    "    data.append([w, min_depth, max_depth])\n",
    "    \n",
    "display_table(data, [\"synset\", \"min_depth\", \"max_depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es decir que si queremos comparar la profundidad relativa entre dos conceptos tenemos que procurar evaluar ambos **utilizando el mismo camino** para no obtener resultados indeseados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Global Wordnet Association](http://globalwordnet.org/): muy interesante la colección de links a *wordnets* en otros idiomas ;D\n",
    " * [Universal Networking Language](https://en.wikipedia.org/wiki/Universal_Networking_Language): otra iniciativa de codificación unívoca del lenguaje. Uno de los grupos de desarrollo está en la UPM-Informática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 62.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 58,-112 58,4 -4,4\"/>\n",
       "<!-- h -->\n",
       "<g id=\"node1\" class=\"node\"><title>h</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">h</text>\n",
       "</g>\n",
       "<!-- v -->\n",
       "<g id=\"node2\" class=\"node\"><title>v</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">v</text>\n",
       "</g>\n",
       "<!-- h&#45;&gt;v -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>h&#45;&gt;v</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-71.6966C27,-63.9827 27,-54.7125 27,-46.1124\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5001,-46.1043 27,-36.1043 23.5001,-46.1044 30.5001,-46.1043\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f46e45683c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
