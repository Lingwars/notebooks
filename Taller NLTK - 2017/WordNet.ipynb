{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "\n",
    " * [¿Qué es WordNet?](Qué-es-WordNet)\n",
    " * [Synsets](#Synsets)\n",
    "    * [Categoría gramatical de un synset](#Categoría-gramatical-de-un-synset)\n",
    "    * [Lo que nos gusta de los synsets](#Lo-que-nos-gusta-de-los-synsets)\n",
    "    * [Lemma](#Lemma)\n",
    " * [Relaciones](#Relaciones)\n",
    "    * [Synset y las relaciones semánticas](#Synset-y-las-relaciones-semánticas)\n",
    "       * [Hiperónimos](#Hiperónimos)\n",
    "       * [Hipónimos](#Hipónimos)\n",
    "       * [Holónimos](#Holónimos)\n",
    "       * [Merónimos](#Merónimos)\n",
    "    * [Lemma y sus relaciones](#Lemma-y-sus-relaciones)\n",
    "       * [Formas derivadas](#Formas-derivadas)\n",
    "       * [Antónimos](#Antónimos)\n",
    " * [Aplicaciones](#Aplicaciones)\n",
    "    * [Expansión de búsquedas](#Expansión-de-búsquedas)\n",
    "    * [Distancia semántica](#Distancia-semántica)\n",
    "    * [Especificidad y concreción](#Especificidad-y-concreción)\n",
    " * [Referencias](#Referencias)\n",
    " \n",
    "Puedes encontrar la última versión de este *workbook* en la cuenta de [Lingẅars](http://lingwars.github.io/blog/) de Github: [https://github.com/Lingwars/notebooks](https://github.com/Lingwars/notebooks/blob/master/Taller%20NLTK%20-%202017/WordNet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet  # This is NLTK's WordNet\n",
    "\n",
    "# Import some helpful classes and functions\n",
    "from IPython.display import HTML, display, Image\n",
    "import graphviz  # pip install graphviz (es una librería para pintar grafos)\n",
    "\n",
    "# This is a function to display a table in HTML format\n",
    "def display_table(data, headers=None, caption=None):\n",
    "    html = [\"<table align=\\\"left\\\">\"]\n",
    "    \n",
    "    if caption:\n",
    "        html += [\"<caption>{}</caption>\".format(caption)]\n",
    "        \n",
    "    if headers:\n",
    "        html += [\"<tr>\"] + [\"<th>{}</th>\".format(h) for h in headers] + [\"</tr>\"]\n",
    "    \n",
    "    for row in data:\n",
    "        html += [\"<tr>\"]\n",
    "        html += [\"<td>{}</td>\".format(it) for it in row]\n",
    "        html += [\"</tr>\"]\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "    display(HTML(''.join(html)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qué es WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WordNet](https://wordnet.princeton.edu/) es una red de conceptos que contiene información codificada manualmente sobre sustantivos, verbos, adjetivos y adverbios en inglés; los términos que representan un mismo concepto están agrupados en *synsets* y son estos elementos los que constituyen los nodos de la red.\n",
    "\n",
    "WordNet se creó en el Laboratorio de Ciencia Cognitiva de la Universidad de Princeton en 1985 bajo la dirección del profesor de psicología George Armitage Miller (1920-2012).\n",
    "\n",
    "Para que nos hagamos una idea de la cantidad de información que contiene ([más números](http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nouns = len(list(wordnet.all_synsets(pos=wordnet.NOUN)))\n",
    "n_verbs = len(list(wordnet.all_synsets(pos=wordnet.VERB)))\n",
    "n_adj = len(list(wordnet.all_synsets(pos=wordnet.ADJ)))\n",
    "n_adv = len(list(wordnet.all_synsets(pos=wordnet.ADV)))\n",
    "\n",
    "print(\"What is inside WordNet?\")\n",
    "print(\"There are {} nouns.\".format(n_nouns))\n",
    "print(\"There are {} verbs.\".format(n_verbs))\n",
    "print(\"There are {} adjectives.\".format(n_adj))\n",
    "print(\"There are {} adverbs.\".format(n_adv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synsets\n",
    "\n",
    "Un synset es un conjunto de palabras de la misma categoría gramatical que hacen referencia a la misma realidad extralingüística y por lo tanto pueden ser intercambiadas en un texto sin afectar al significado. Son elementos semánticamente equivalentes. Así, ocurrirá que las palabras polisémicas aparecerán múltiples veces en *synsets* diferentes.\n",
    "\n",
    "Podemos hacer una búsqueda de uno de estos *synsets* utilizando la función `synsets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_synsets = wordnet.synsets('dog')\n",
    "print(\"There are {} synsets referring to this word:\".format(len(my_synsets)))\n",
    "\n",
    "data = []\n",
    "for synset in my_synsets:\n",
    "    data.append([synset.name(), synset.definition()])\n",
    "\n",
    "display_table(data, [\"Synset\", \"Definition\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos quedarnos con uno de ellos y explorar la cantidad de información que ofrece WordNet una vez que hemos encontrado el *synset* que nos interesa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_synset = my_synsets[0]  # TODO: Prueba con otros: my_synsets[1], my_synsets[2],...\n",
    "print(\"synset.name: {}\".format(my_synset.name()))\n",
    "print(\"synset.definition: {}\".format(my_synset.definition()))\n",
    "\n",
    "print(\"synset.examples:\")\n",
    "for example in my_synset.examples():\n",
    "    print(\"\\t + {}\".format(example))\n",
    "\n",
    "print(\"synset.lemmas:\")\n",
    "for lemma in my_synset.lemmas():\n",
    "    print(\"\\t + {}\".format(lemma.name()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y también podemos buscar los lemmas correspondientes a un *synset* en otros idiomas. Vamos a mostrar aquí sólo un ejemplo porque trataremos este tema más adelante. Véamos cuáles son los relacionados con el *synset* que hemos guardado en la variable `my_synset` de la celda anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = sorted(wordnet.langs())\n",
    "print(\"These are the languages available: {}\".format(', '.join(languages)))\n",
    "\n",
    "selected_languages = ['eng', 'spa', 'fra', 'eus', 'jpn',] # TODO: Prueba con otros idiomas (de la lista)\n",
    "\n",
    "data = []\n",
    "for lang in selected_languages:\n",
    "    data.append([lang, '</br>'.join(my_synset.lemma_names(lang))])\n",
    "\n",
    "display_table(data, headers=[\"lang\", \"lemmas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoría gramatical de un synset\n",
    "\n",
    "En el apartado anterior hemos recuperado todos los *synsets* a partir de una palabra y nos han aparecido significados correspondientes a sustantivos, verbos, adjetivos,... pero se puede afinar un poco más la búsqueda utilizando el *part of speech (pos)*:\n",
    "\n",
    " * `wordnet.VERB`\n",
    " * `wordnet.NOUN`\n",
    " * `wordnet.ADJ`\n",
    " * `wordnet.ADV`\n",
    " \n",
    "¡Vamos a verlo en acción!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"bien\"  # TODO: seguro que se te ocurren palabras que puedan aparecer en varias categorías gramaticales\n",
    "lang = \"spa\"  # TODO: estamos buscando en español, pero...\n",
    "\n",
    "synsets_as_noun = wordnet.synsets(word, lang=lang, pos=wordnet.NOUN)\n",
    "synsets_as_verb = wordnet.synsets(word, lang=lang, pos=wordnet.VERB)\n",
    "synsets_as_adj = wordnet.synsets(word, lang=lang, pos=wordnet.ADJ)\n",
    "synsets_as_adv = wordnet.synsets(word, lang=lang, pos=wordnet.ADV)\n",
    "\n",
    "# Vamos a imprimir los resultados\n",
    "print(\"synsets_as_noun: {}\".format(synsets_as_noun))\n",
    "\n",
    "# TODO: ¿Te atreves a mostrar más información sobre ellos con una tabla?:\n",
    "# def synset_table(synsets, title):\n",
    "#    data = []\n",
    "#    for synset in synsets:\n",
    "#        data.append([synset.name(), synset.lemma_names(), synset.definition()])\n",
    "#        \n",
    "#    display_table(data, [\"Synset\", \"Lemmas\", \"Definition\"], caption=title)\n",
    "#\n",
    "#synset_table(synsets_as_noun, title=\"Resultados con: wordnet.NOUN\")\n",
    "#synset_table(synsets_as_verb, title=\"Resultados con: wordnet.VERB\")\n",
    "#synset_table(synsets_as_adj,  title=\"Resultados con: wordnet.ADJ\")\n",
    "#synset_table(synsets_as_adv, title=\"Resultados con: wordnet.ADV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo que nos gusta de los *synsets*\n",
    "\n",
    "Lo que nos gusta de los *synsets* es que permiten referirse a un significado sin ambigüedades. A los ordenadores se les da muy mal resolver ambigüedades, generalmente se les da muy mal todo lo que humanos hacemos con relativa facilidad (entender un mensaje, reconocer imágenes,...), pero realizan muy eficazmente tareas que a nosotros nos cuestan mucho tiempo (búsquedas, ordenación,...).\n",
    "\n",
    "Los ordenadores querrían ver los textos de esta forma, **sin ambigüedades**:\n",
    "\n",
    "```\n",
    "El perro ladra ---> El dog.n.01 bark.v.04\n",
    "```\n",
    "así podrían entenderlo y podríamos hacer inferencias sin equivocarnos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma\n",
    "\n",
    "No debe confundirse un **synset** con un **lemma**, tal y como los identifica WordNet. Recordemos que:\n",
    " \n",
    " * un **synset** está asociado a un significado, que puede representarse en lenguaje natural mediante palabras (lemmas) muy diferentes: perro, dog, can,...\n",
    " * un **lemma** es una palabra de lenguaje natural y, por lo tanto, puede tener varios significados (synsets).\n",
    " \n",
    "Esta diferencia es importantísima tenerla presente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_synset = wordnet.synset('dog.n.01')\n",
    "for lemma in ex_synset.lemmas():\n",
    "    print(\"{} -> {}\".format(ex_synset, lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, a traves de un *synset* llegamos a lemmas diferentes, pero todos ellos con el mismo significado. De hecho, el identificador del sysnset `dog.n.01` se mantiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_lemmas = wordnet.lemmas(\"dog\")\n",
    "for lemma in ex_lemmas:\n",
    "    print(\"{} -> {} -> {}\".format(lemma.name(), lemma, lemma.synset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, cuando buscamos una palabra obtenemos varios lemmas, cada uno de ellos asociado a un synset diferente. Se puede observar cómo los identificadores de los synsets son diferentes: `dog.n.01`, `frump.n.01`,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué tal vamos hasta aquí?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bien = \"https://media.giphy.com/media/tqxGgrCnQGsHm/giphy.gif\"\n",
    "mal = \"https://media.giphy.com/media/rn0rRhia7343u/giphy.gif\"\n",
    "\n",
    "# TODO: Selecciona la imagen que corresponda (sustituye la variable 'img' por una de las de arriba)\n",
    "# display(HTML(\"<img src='{}' />\".format(img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relaciones\n",
    "\n",
    "Como decíamos al principio, WordNet es más que un diccionario o un traductor, se trata de una **red de conceptos** que nos permite buscar relaciones entre significados de una forma extremadamente fácil e interesante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset y las relaciones semánticas\n",
    "\n",
    "Los elementos de tipo *synset* definen algunas relaciones que puedes explorar. A continuación te indicamos cuáles creemos que son las más interesantes (puedes consultar la lista completa [aquí](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset)):\n",
    "\n",
    " * hiperónimos\n",
    " * hipónimos\n",
    " * holónimos\n",
    " * merónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<strong>Relaciones de hiperonimia e hiponimia</strong>\"))\n",
    "d = graphviz.Digraph()\n",
    "d.edge(\"furniture.n.01\", \"bookcase.n.01\")\n",
    "d.edge(\"furniture.n.01\", \"cabinet.n.01\")\n",
    "d.edge(\"furniture.n.01\", \"table.n.02\")\n",
    "d.edge(\"table.n.02\", \"altar.n.01\")\n",
    "d.edge(\"table.n.02\", \"table-tennis_table.n.01\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La hiperonimia e hiponimia codifican relaciones a nivel de significado. Un **hipónimo** concreta el significado de su **hiperónimo**, así mesa es más específico que mueble, como altar es un tipo particular de mesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<strong>Relaciones de holonimia y meronimia</strong>\"))\n",
    "\n",
    "d = graphviz.Digraph()\n",
    "d.graph_attr['rankdir'] = 'LR'\n",
    "d.edge(\"car.n.01\", \"air_bag.n.01\")\n",
    "d.edge(\"car.n.01\", \"car_door.n.01\")\n",
    "d.edge(\"car.n.01\", \"gasoline_engine.n.01\")\n",
    "d.edge(\"car_door.n.01\", \"doorlock.n.01\")\n",
    "d.edge(\"car_door.n.01\", \"hinge.n.01\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las relaciones de holonimia y meronimia codifican relaciones entre el todo y sus partes. Un puerta (de coche) es una parte del coche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_synset = wordnet.synset(\"hand.n.01\")  # TODO: Cambia el valor almacenado en la variable 'my_synset' para \n",
    "                                         #   obtener otros resultados en las siguientes celdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for hypernym in my_synset.hypernyms():\n",
    "    lemmas = [lemma.name() for lemma in hypernym.lemmas()]\n",
    "    data.append([hypernym, ', '.join(lemmas), '</br>'.join(hypernym.examples())])\n",
    "display_table(data, [\"hyp-synset\", \"lemmas\", \"examples\"], caption=\"Hypernyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de los hiperónimos a nivel de significado, también existen **a nivel de instancia `instance_hypernyms`** (igual con los hipónimos). Por ejemplo, si hemos encontrado en un texto una entidad (ver NER) como `Vargas Llosa`, gracias a WordNet podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Zweig\"  # TODO: Prueba otros como p.ej.: Vargas_Llosa, Zweig, Einstein\n",
    "\n",
    "ner = wordnet.synsets(q)[0]  # Recoge el primer resultado\n",
    "hiperonimos = ner.instance_hypernyms()[0]  # Busca sus hiperónimos y quédate con el primero\n",
    "\n",
    "# Qué tengo?\n",
    "print(\"Había buscado: {}\".format(q))\n",
    "print(\"{} is a {}\".format(ner.name(), hiperonimos.name()))  # Su hiperónimo me dice su profesión!!\n",
    "print(\"Otras formas de llamarlo son: {}\".format(', '.join([it.name() for it in ner.lemmas()])))\n",
    "print(\"Su 'definición' es: {}\".format(ner.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos casos, por ejemplo, el hiperónimo de un un escritor concreto es la categoría *escritor* puesto que generaliza su significado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hipónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for hyponym in my_synset.hyponyms():\n",
    "    lemmas = [lemma.name() for lemma in hyponym.lemmas()]\n",
    "    data.append([hyponym, ', '.join(lemmas), '</br>'.join(hyponym.examples())])\n",
    "display_table(data, [\"hyponym-synset\", \"lemmas\", \"examples\"], caption=\"Hyponyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# member_holonyms, substance_holonyms, part_holonyms\n",
    "data = []\n",
    "for holonym in my_synset.part_holonyms():\n",
    "    lemmas = [lemma.name() for lemma in holonym.lemmas()]\n",
    "    data.append([holonym, ', '.join(lemmas), '</br>'.join(holonym.examples())])\n",
    "display_table(data, [\"holonym-synset\", \"lemmas\", \"examples\"], caption=\"Holonyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# member_meronyms, substance_meronyms, part_meronyms\n",
    "data = []\n",
    "for meronym in my_synset.part_meronyms():\n",
    "    lemmas = [lemma.name() for lemma in meronym.lemmas()]\n",
    "    data.append([meronym, ', '.join(lemmas), '</br>'.join(meronym.examples())])\n",
    "display_table(data, [\"meronym-synset\", \"lemmas\", \"examples\"], caption=\"Meronyms for {}\".format(my_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma y sus relaciones\n",
    "\n",
    "Los lemas, por su parte, también definen unas cuantas relaciones interesantes (lista completa [aquí](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Lemma)), si bien, las dos que nos parecen más interesantes son:\n",
    "\n",
    " * antonyms\n",
    " * derivationally_related_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_lemma = wordnet.lemma(\"fast.a.01.fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formas derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang='spa'  # TODO: Change the language\n",
    "data = []\n",
    "for item in my_lemma.derivationally_related_forms():\n",
    "    lemmas = item.synset().lemma_names(lang=lang)\n",
    "    data.append([item, '</br>'.join(lemmas), '</br>'.join(item.synset().examples())])\n",
    "display_table(data, [\"derivationally-related-forms\", \"lemmas\", \"examples\"], caption=\"Derivates for {}\".format(my_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang='eng'  # TODO: Change the language\n",
    "data = []\n",
    "for item in my_lemma.antonyms():\n",
    "    lemmas = item.synset().lemma_names(lang=lang)\n",
    "    data.append([item, '</br>'.join(lemmas), '</br>'.join(item.synset().examples())])\n",
    "display_table(data, [\"antonym-lemma\", \"lemmas\", \"examples\"], caption=\"Antonym for {}\".format(my_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las relaciones entre un elemento y sus vecinos permiten explorar la red de conceptos buscando términos relacionados con una palabra (lema) dada o bien con un concepto (synset) determinado. Sin embargo, esta estructura de relaciones nos permite también abordar problemas mucho más ambiciosos. Algunos de ellos los vemos a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Expansión de búsquedas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las primeras aplicaciones que se nos pueden ocurrir y quizá de las más utilizadas es la expansión de búsquedas. Para ello podemos utilizar estas redes de conceptos para ampliar la búsqueda utilizando otras palabras relacionadas. De este modo lo que se persigue es encontrar documentos que, aunque no contengan exactamente la misma palabra que se ha introducido, sí sean relevantes por contener otras relacionadas.\n",
    "\n",
    "Por ejemplo, si el usuario ha introducido `dalmatian` como término de búsqueda, podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"dalmatian\"\n",
    "print(\"Original query: {}\".format(q))\n",
    "\n",
    "# Look synsets related to the keyword\n",
    "synsets = wordnet.synsets(q)\n",
    "\n",
    "# Expand the query to all related synsets\n",
    "def gather_lemmas(synset_list):\n",
    "    lemmas = []\n",
    "    for synset in synset_list:\n",
    "        lemmas += synset.lemma_names()\n",
    "    return lemmas\n",
    "    \n",
    "expanded_query = [q] + gather_lemmas(synsets)\n",
    "print(\"Expanded to synsets: {}\".format(', '.join(set(expanded_query))))\n",
    "\n",
    "# Expand the query to all hyperonyms:\n",
    "for synset in synsets:\n",
    "    expanded_query += gather_lemmas(synset.hypernyms())\n",
    "\n",
    "print(\"Expanded with hyperonyms: {}\".format(', '.join(set(expanded_query))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al tener muchos más términos de búsqueda podremos recuperar muchos más documentos de nuestro corpus en caso de que la búsqueda original ofreciera un numero insuficiente de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra aplicación muy habitual cuando se dispone de una red de conceptos es medir la distancia semántica entre significados. Algunas situaciones en las que puede plantearse esta necesidad son la evaluación de traductores (cuál se ha separado menos del significado original) o la desambiguación (ante un mismo lema con varios significados podemos asignarle una probabilidad a cada uno de ellos según la distancia a la temática del documento).\n",
    "\n",
    "Esta aplicación es tan demandada que NLTK implementa los principales algoritmos para calcular esta medida. Por ejemplo, dados tres significados `dog.n.01`, `cat.n.01` y `tiger.n.01`, veamos cuál es la distancia entre ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset1 = wordnet.synset(\"dog.n.01\")\n",
    "synset2 = wordnet.synset(\"cat.n.01\")\n",
    "sim = wordnet.path_similarity(synset1, synset2)\n",
    "\n",
    "print(\"The similarity between {} and {} using the given algorithm is {:0.4f}\".format(synset1, synset2, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir ahora una lista en las que incorporaremos todas las funciones que ofrece WordNet para el cálculo de la similaridad entre dos *synsets* (en las [referencias](#Referencias) se incluye un documento en el que se detallan estos algoritmos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Métodos básicos para calcular similitud entre dos términos\n",
    "methods = [('path_similarity', wordnet.path_similarity),\n",
    "           ('Leacock-Chodorow', wordnet.lch_similarity),\n",
    "           ('Wu-Palmer', wordnet.wup_similarity),]\n",
    "\n",
    "# Algunos algoritmos necesitan un 'corpus' para utilizarlo como referencia\n",
    "from nltk.corpus import wordnet_ic, genesis\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "genesis_ic = wordnet.ic(genesis, False, 0.0) # Esto puede tardar un poco la primera vez\n",
    "\n",
    "methods_ic = [('Resnik + Brown', lambda u,v: wordnet.res_similarity(u, v, brown_ic)),\n",
    "              ('Resnik + Semcor', lambda u,v: wordnet.res_similarity(u, v, semcor_ic)),\n",
    "              ('Resnik + Genesis', lambda u,v: wordnet.res_similarity(u, v, genesis_ic)),\n",
    "                \n",
    "              ('Jiang-Conrath + Brown', lambda u,v: wordnet.jcn_similarity(u, v, brown_ic)),\n",
    "              ('Jiang-Conrath + Semcor', lambda u,v: wordnet.jcn_similarity(u, v, semcor_ic)),\n",
    "              ('Jiang-Conrath + Genesis', lambda u,v: wordnet.jcn_similarity(u, v, genesis_ic)),\n",
    "           \n",
    "              ('Lin + Brown', lambda u,v: wordnet.lin_similarity(u, v, brown_ic)),\n",
    "              ('Lin + Semcor', lambda u,v: wordnet.lin_similarity(u, v, semcor_ic)),\n",
    "              ('Lin + Genesis', lambda u,v: wordnet.lin_similarity(u, v, genesis_ic)),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también una función para mostrar los resultados de nuestros cálculos en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float_format = \"{0:.4f}\"\n",
    "\n",
    "def compute_distances(item_list, method_list):\n",
    "    data = []\n",
    "    it1 = item_list[0]\n",
    "    for key,method in method_list:\n",
    "        max_similarity = method(it1, it1)\n",
    "        row = [float_format.format(1.0),]\n",
    "\n",
    "        for it2 in item_list[1:]:\n",
    "            similitud = method(it1, it2)/max_similarity  # Normalized by 'similarity(it1, it1)'\n",
    "            row.append(float_format.format(similitud)) \n",
    "        data.append([key] + row)\n",
    "\n",
    "    columns = [\"{} > {}\".format(item_list[0].name(), it.name()) for it in item_list]\n",
    "    display_table(data, [\"Method\"] + columns, caption=\"Similitud normalizada entre '{}'\".format(it1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los elementos anteriores, la lista de algoritmos y la función auxiliar, podemos empezar a calcular cosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wordnet.synset(\"dog.n.01\")\n",
    "cat = wordnet.synset(\"cat.n.01\")\n",
    "tiger = wordnet.synset(\"tiger.n.01\")\n",
    "animals = [dog, cat, tiger] # TODO: Aquí puedes añadir más synsets\n",
    "\n",
    "compute_distances(animals, methods + methods_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1 = wordnet.synset(\"Einstein.n.01\")\n",
    "p2 = wordnet.synset(\"Austen.n.01\")\n",
    "p3 = wordnet.synset(\"Zweig.n.01\")\n",
    "p4 = wordnet.synset(\"Cousteau.n.01\")\n",
    "p5 = wordnet.synset(\"akhenaton.n.01\")\n",
    "people = [p1, p2, p3, p4, p5] # TODO: Aquí puedes añadir más synsets\n",
    "\n",
    "for it in people:\n",
    "    hyp = it.instance_hypernyms()[0]\n",
    "    print(\"{} is a {}\".format(it.lemmas()[0].name(), hyp.lemmas()[0].name()))\n",
    "\n",
    "compute_distances([p1, p2, p3, p4, p5], methods)\n",
    "compute_distances([p3, p1, p2, p4, p5], methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desambiguación\n",
    "\n",
    "El problema clásico y ¿sin solución definitiva? del Procesamiento de Lenguaje Natural: desambiguar el significado de una palabra. Una forma de abordarlo que se me ocurre con lo que sabemos hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"El banco presta dinero a cambio de un interés\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos tokenizar y realizar el análisis sintáctico de una oración. Con ello obtendremos las palabras individuales y el *part of speech* de cada una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = [(\"bank\", \"n\"), (\"lend\", \"v\"), (\"money\", \"n\"), (\"interest\", \"n\")]\n",
    "\n",
    "# Remove everything but nouns (WordNet does not retrieve similarity for different part of speech synsets)\n",
    "sentence = [(it, pos) for it, pos in sentence if pos==\"n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente tenemos un problema de desambiguación puesto que cada una de estas palabras puede tener diferentes significados. Vamos a listarlos todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, pos in sentence:\n",
    "    display(HTML(\"<strong>Significados de {}:</strong>\".format(word)))\n",
    "\n",
    "    synsets = wordnet.synsets(word, pos=pos)\n",
    "    data = []\n",
    "    for s in synsets:\n",
    "        data.append([s, ', '.join([it.name() for it in s.lemmas()]), s.definition()])\n",
    "        \n",
    "    display_table(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La estrategia que vamos a seguir es probar todas las combinaciones posibles y quedarnos con aquélla que ofrezca la mayor similitud entre sus componentes. ¿Aproximación válida? ¿Inválida? ¿Muy costosa computacionalmente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_options = [wordnet.synsets(word, pos=pos) for word, pos in sentence]\n",
    "\n",
    "import itertools\n",
    "all_combinations = list(itertools.product(*sentence_options))\n",
    "display(HTML(\"There are {} possible combinations!!\".format(len(all_combinations))))\n",
    "\n",
    "data = {}\n",
    "for comb in all_combinations:\n",
    "    sim = 0\n",
    "    for pair in itertools.combinations(comb, r=2):\n",
    "        if pair[0].pos() == pair[1].pos():  # WordNet does not return similarity between different part-of-speech\n",
    "            sim += wordnet.path_similarity(pair[0], pair[1])\n",
    "    data[comb] = sim\n",
    "\n",
    "import operator\n",
    "sorted_data = sorted(data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Keep just the first 10\n",
    "display(HTML(\"Top 10 are:\"))\n",
    "top10 = sorted_data[:10]\n",
    "\n",
    "# Vamos a imprimir los lemmas, aunque un ordenador lo que querría son los synsets.\n",
    "data_to_display = []\n",
    "for it, sim in top10:\n",
    "    row = [\"{:0.4f}\".format(sim)]\n",
    "    for synset in it:\n",
    "        cell_text = \"{}: {}\".format(synset.name(), ', '.join([lema.name() for lema in synset.lemmas()]))\n",
    "        row.append(cell_text)\n",
    "    data_to_display.append(row)\n",
    "\n",
    "display_table(data_to_display, headers=[\"score\"] + [word for word, pos in sentence])\n",
    "\n",
    "# And the winner is\n",
    "winner, sim = top10[0]\n",
    "display(HTML(\"<strong>And the winner is</strong>, with a score of {}:<ul>\".format(sim)))\n",
    "for it in winner:\n",
    "    display(HTML(\"<li><strong>{}</strong>: {}</li>\".format(it.name(), it.definition())))\n",
    "display(HTML(\"</ul>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo podemos mejorar este resultado de manera fácil? Si tenemos información sobre el contexto, por ejemplo, si sabemos que es una noticia de la sección económica de un periódico, entonces podemos utilizar como input la frecuencia relativa de nuestros significados dentro de ese corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificidad y concreción\n",
    "\n",
    "¡Toma epígrage! Hay un método que te devuelve la profundidad en la jerarquía de conceptos de una palabra, ¿se podría evaluar en base a esto cómo de riguroso es un autor?\n",
    "\n",
    "En primer lugar, ten en cuenta que WordNet no es una estructura en forma de árbol, sino que es un grafo, es decir, ¡puede haber varios caminos para llegar a un mismo nodo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wordnet.synset('dog.n.01')\n",
    "\n",
    "for path in dog.hypernym_paths():\n",
    "    print(\"== Path:\")\n",
    "    print(' >> '.join([item.name() for item in path]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es importante porque al calcular la profundidad de un nodo en la jerarquía de conceptos obtendremos varios valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {'dog': 'dog.n.01',\n",
    "          'dalmatian': 'dalmatian.n.02',\n",
    "          'animal': 'animal.n.01'}\n",
    "\n",
    "data = []\n",
    "for w,s in words.items():\n",
    "    min_depth = wordnet.synset(s).min_depth()\n",
    "    max_depth = wordnet.synset(s).max_depth()\n",
    "    data.append([w, min_depth, max_depth])\n",
    "    \n",
    "display_table(data, [\"synset\", \"min_depth\", \"max_depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es decir que si queremos comparar la profundidad relativa entre dos conceptos tenemos que procurar evaluar ambos **utilizando el mismo camino** para no obtener resultados indeseados. Mira la jerarquía de conceptos para las palabras anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for w,s in words.items():\n",
    "    synset = wordnet.synset(s)\n",
    "    paths = synset.hypernym_paths()\n",
    "    for path in paths:\n",
    "        names = [it.name() for it in path]\n",
    "        \n",
    "        # Iterate in pairs\n",
    "        for v, w in zip(names[:-1], names[1:]):\n",
    "            edges.append((v, w))\n",
    "\n",
    "# Remove duplicates\n",
    "edges = set(edges)\n",
    "\n",
    "# Plot graph\n",
    "g = graphviz.Digraph()\n",
    "for v, w in edges:\n",
    "    g.edge(v, w)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Global Wordnet Association](http://globalwordnet.org/): muy interesante la colección de links a *wordnets* en otros idiomas ;D\n",
    " * [Universal Networking Language](https://en.wikipedia.org/wiki/Universal_Networking_Language): otra iniciativa de codificación unívoca del lenguaje. Uno de los grupos de desarrollo está en la UPM-Informática.\n",
    " * \"[Medida de distancia semántica en grafos UNL](https://www.dropbox.com/s/pygvolndftoshz9/GarciaSogo_Javier%20-%20Medida%20de%20distancia%20sem%C3%A1ntica%20en%20grafos%20UNL.pdf?dl=0)\" (Javier G. Sogo, 2015): utilizando UNL y WordNet se propone una metodología para evaluar la distancia semántica entre dos oraciones y así valorar el funcionamiento de dos traductores automáticos. Incluye documentación sobre los algoritmos de distancia semántica utilizados en la ontología de WordNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
